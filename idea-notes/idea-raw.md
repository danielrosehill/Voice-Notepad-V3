# Voice Notepad V3 - Raw Transcript

Okay, so uh here are some notes for context uh for what I want to try in this uh iteration of the voice notepad idea, which remains a central uh tool that I've been working on and going through some different iterations with between local um inference and remote inference and probably tending mostly as in this example coming back to uh remote inference or using cloud APIs basically in order to um transcribe. Now, what I'd like to do in this particular uh project is the central tool that I'm looking for and uh use every day is a voice notepad. So basically, um you know, a UI for Ubuntu Linux. Um could probably easily be adapted to other platforms uh with functions for record recording your voice, so record, pause, stop, delete, uh basic recording controls, microphone selection, and then sending that for transcription um and it appearing into a text box. That's the fundamental um thing that we want to to do. Now, what I'm want to do in this one as opposed to previous versions, which I've been using speech to text models, um is using audio multimodal models that support the audio that support audio as an input modality. And what I want to do is combine the transcription with an instruction for cleaning up the text. Um which is the single phase that was two phases in the previous one, which was ASR and then large language model. So that's why I want to create this one. Um it's a consolidated cleanup tool. So there's I went created a list earlier this week of different um different models that are out there that support this and in order to in order for this to work, um you need a multimodal model with which supports audio um token processing as well as prompt processing. And what I'd like for this um this tool is that we should have a standard, what I call a cleanup prompt that's baked in. Um and it's approximately this. Your task is to um provide a cleaned transcription of the audio recorded by the user, remove filler words, add sentences and add natural paragraph spacing. And if the user makes any instructions in the course of the transcription, such as um don't include this or change this, um include that in the transcription and add subheadings if it's a length lengthy transcription. So the idea, that's the end of the prompt. So the idea is that there there'll be in the background this kind of standard guiding system prompt and that gets sent together with the audio to the model. So there is two ones that I'd like to use as starters that I know work reliably well. The first is an initially create these as um .env.env variables in the repository. But the ultimate objective is that we'll build us out to a Debian that anyone can use and therefore we should build the app with um some basic system for storing uh safely these keys on the user's local file system. So, but we'll start with Gemini and Open AI. Um, I'll provide the exact model names because that's very important as they've recently changed. Uh for Gemini, we can use Gemini 2.5 Light preview. In fact, it would be good to give the user um an options. So, I'll provide the actual APIs for Gemini and for Open AI. Um, the third one that I'd like to offer, and there is in in my list that I gathered this week, the ones are kind of viable outside of Gemini and Open AI, the challenge is it becomes a little bit harder to find um inference providers. And sometimes the these audio multimodal models, the only way to get that kind of the workflow I'm describing where you have a audio data together with a prompt is actually to use a chat endpoint and they maintain a classic um Omni endpoint. Um for sorry, they maintain a classic audio transcription endpoint, which will just do raw transcription, which again kind of does defeats the purpose from this implementation standpoint. Um but the third one that I think would be useful to include is a model called Voxl, v o x t r a l. Um this was released by Mistral AI and I looked at their API doc yesterday and it seems that you I think it's the chat endpoint that's required. But it's very cost effective and it's uh built for this. So, what I'd like to do is if it's possible if we can get the first two running, let's also add Voxl support. Um and let's use the larger of their two models, the 24B one. I think it's called small. And we'll use the Mistral API directly, so that would require of course a Mistral API key. Um and that would round off the initial implementation. Um and this is designed for use for kind of quick use. I'm on a Wayland Linux, KDE Plasma. Um, it would be great if we have the ability to as a mode setting to um type to insert the uh text into the into the um, you know, a place on any anywhere the user has text. But that in my experience is a little challenging with getting virtual inputs to work. But I'm just noting it as a down the line feature and the other feature that might be more immediately viable and would be very useful would be automatically copy into clipboard. But I think by default and at least in the initial one, let's just have it goes into a text box and there's just a button to copy and finally, um we should have the ideas this is a tool that would be used kind of very very, very frequently um throughout the day and something like just to make it easy to start a new note. The final the the final thing I keep adding on little ideas is cost tracking because um it's obviously going to infer cost to be using this. Um I don't know how easy it is to get if let's assume there's a unique API key in Gemini and an Open AI for this app. So we can just say everything this key is spending is app activity. Um, and then we could have maybe like spend today. Yeah, spend today, spend this week, spend this month to to have people to uh just to allow the users to kind of keep on top of their spend and know what they're spending. But that again is kind of more like a little bit of a down the line feature I'd say if it's hard to integrate, but I'm just capturing all all of these um in the initial specs so we can just uh have a have a wide definition and then iterate upon these features. So the core ones to start with would just be the transcription and I'll add my API keys and let's start developing it.
