# Voice Note: Second Pass Processing (Raw Transcript)

**Recorded:** 31 Dec 2025

---

Okay, this um is the first voice note in the uh repository. Today is the 31st of December 2025. And uh this is the voice notepad, uh as it currently stands. I've tried to get a number of these running over the course of the past year. And this is um probably the the the best implementation so far. And actually the one that I actually now I feel like I'm I'm sticking as in the others were kind of proof of concepts and this is actually something that I'm really building out and using. I've trying to call up my uh the analytics uh that I have, and today's one is, wow, 270 207,000 words I've transcribed to date using this app. And I've done 200 2,383 transcriptions. So uh quite a sizable uh test and it's been extremely cost effective and extremely extremely useful. So these these notes are planning out um at this point everything all the basics are in place in terms of what I wanted to get from it. And these now I now I have the luxury of being able to think of little other bits and pieces to kind of uh to add in. So here's the first one of those. Uh, we can call this text correction. So the prompt, the concatenated prompt that gets sent to Gemini for um that gets constructed through the prompt stack, defining format, tone, style, or in most cases just the general prompt, um works in general very, very well. And but there's edge cases of failure. The failures are basically all related to uh getting keywords wrong or names wrong that might be probably attributable to sloppy pronunciation on my part. But as an example, Claude code, frequently referring to that tool. If I say it slowly, like Claude code, it will probably get it right. If I say Claude code, or anyway, it commonly comes back as Clyde code, c l y d. Another one is open router, as in the API tool for routing requests to large language models. That frequently comes back as open Ryder, r i d e r. So there is a couple of ones that are kind of commonplace enough and recurrent enough that um that would motivate me to actually create a rule. Uh, the reason that I don't want to do that traditional approach is because I guess this whole app is kind of um taking a more a slightly more uh unique direction for transcription, starting with using a multi-modal model instead of an ASR model. And likewise, my issue with the the word replacement tools are that trying to use this stuff every day. Um they're they're a bit clunky because you they're not a lot of fun to maintain and update your your error list. Um they tend not to be portable. And so I don't want to create another tool like that. The solution that sort of comes to my mind at the moment is the the other issue I think with those predictive that method is that it's ver it's impossible to predict what these tools are going to get wrong. Sometimes it's it's surprising. Um so if I just define a list of predictions or word replacements, it's always going to be kind of imperfect and I'll always have to be playing catch up with random mistakes that the model makes in transcription. I think that a smarter approach is second pass a uh large language model. I think there's only so much that can be done with system prompting and telling the model, you know, just to do transcribe stuff accurately. It's still going to make these mistakes. The second pass there's a few options here and I'm just the reason that I'm creating this as a node is to kind of just jot them down and jot them out. Um the first one, the heaviest the heaviest one from a API perspective and is just to send it back. Sending it back meaning sending the original audio together with the first transcript back to Gemini. Saying or whatever, saying, you got stuff wrong. Pay attention to what the user said. Try harder, basically. Um that's implementation one. Implementation two is not sending the audio and just sending the text and saying Gemini, something about this text contains a something that you transcribed from audio and you got it wrong. Can you can you use your reasoning to figure out what that is? And I might say, oh yeah, open writer in the context of a discussion about coding, that should definitely have been open router and it'll send back the corrected transcript. Um I tend between those two, I tend towards version two because firstly, it's going to be quicker. It's also going to be more cost effective, not that that these the cost here are very marginal. But quicker is the big is the big uh draw for that one for me that you that it could be like a fixed button that the user uses without having to when you have to add a prompt like you say what's wrong with the text. At that point, you may as well just fix it yourself. If it's a a quick if it's a lightning quick button that just says fix, it sends it up in a flash, sends out prompt text up and down. That's a case where it might be helpful. Uh, the final one and I as I say this, I tend to the the third one that I'll just add for the send the sake of completion, but I don't like it. I don't like the idea of it. I'm trying to avoid this is a cloud AI tool. I don't really like using local stuff for voice that much. Although we're using local VAD. Um, a small model basically. So a not sending it up to something big like Gemini for this task, using something small. And I think that probably makes more sense for the second option, actually. Sending it to the same model again is almost counterintuitive. It's saying you got it wrong, the same guy, you know, have it have a second go. Um but I think it could be a small model like Llama or something very, you know, fine tuned on identifying mistranscriptions by surrounding context and fixing them. Um and that's probably the one that makes the most sense to me. That would add speed again over Gemini and it's another one of those edge cases where it probably could be run locally. But doing so then mean means it's more complicated to set this up because you have another model to pull, install. VAD is already local. That one makes sense. VAD makes sense locally. I'm not sure that this makes sense. Um but that would involve adding one more tool to the pipeline. Um the final idea here is to kind of try to make preventive a preventive uh version of this, which is that every single transcription, or there's an option to send every single transcription for a second pass processing, that and that would be a slightly different implementation. There is an important subtlety, it's important nuance that in the second one, the second model would have to know that in all likelihood, there isn't anything wrong. But if if there is, do it. Now, the problem with that from an AI engineering standpoint is it the models are biased towards making fixes. And that's a hard behavior to mitigate against. And you run the risk in that approach, in my experience of getting a second pass model, a small model that just tears up all the good work of the first model and is a disaster. So those are the options. First, I guess significant feature as an add-on to be explored.
